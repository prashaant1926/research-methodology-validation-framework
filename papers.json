[
  {
    "id": "ioannidis-2005-false",
    "title": "Why Most Published Research Findings Are False",
    "authors": "John P. A. Ioannidis",
    "journal": "PLOS Medicine",
    "year": "2005",
    "url": "https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124",
    "hypotheses": "Universal failure patterns, Design-stage intervention effectiveness",
    "notes": "Foundational paper establishing that most published research findings are false due to systematic methodological issues",
    "strengths": "Mathematical framework, broad applicability, predictive power",
    "weaknesses": "Limited empirical validation of proposed solutions at time of publication",
    "citation": "Ioannidis, J. P. A. (2005). Why most published research findings are false. PLOS Medicine, 2(8), e124.",
    "problem": "Most published research findings are false due to systematic biases in study design, analysis, and publication",
    "assumption_prior_work": "Research validity could be ensured through peer review and replication after publication",
    "insight": "Research findings' truth probability depends on study power, bias, and the ratio of true to false relationships being tested",
    "impact": "Catalyzed the reproducibility crisis movement and meta-science field development"
  },
  {
    "id": "open-science-collab-2015",
    "title": "Estimating the reproducibility of psychological science",
    "authors": "Open Science Collaboration",
    "journal": "Science",
    "year": "2015",
    "url": "https://science.sciencemag.org/content/349/6251/aac4716",
    "hypotheses": "Universal failure patterns",
    "notes": "Large-scale replication study showing only 39% of 100 psychology studies could be replicated",
    "strengths": "Systematic methodology, large sample, transparent process",
    "weaknesses": "Limited to psychology, single replication attempts",
    "citation": "Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. Science, 349(6251), aac4716.",
    "problem": "Unknown replication rate in psychological science and factors affecting reproducibility",
    "assumption_prior_work": "Published psychological findings are generally reliable and replicable",
    "insight": "Systematic replication reveals much lower success rates than expected and identifies key moderating factors",
    "impact": "Provided concrete evidence for reproducibility crisis and influenced journal and funding policies"
  },
  {
    "id": "hardwicke-2020-metaresearch",
    "title": "Calibrating the Scientific Ecosystem Through Meta-Research",
    "authors": "Tom E. Hardwicke, Stylianos Serghiou, Perrine Janiaud, Valentin Danchev, Sophia Crüwell, Steven N. Goodman, John P.A. Ioannidis",
    "journal": "Annual Review of Statistics and Its Application",
    "year": "2020",
    "url": "https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-031219-041104",
    "hypotheses": "Cross-field transferability, Design-stage intervention effectiveness",
    "notes": "Systematic approach to evaluating research transparency and reproducibility using quantifiable methodologies",
    "strengths": "Comprehensive framework, evidence-based approach, practical tools",
    "weaknesses": "Implementation challenges, resource requirements",
    "citation": "Hardwicke, T. E., et al. (2020). Calibrating the scientific ecosystem through meta-research. Annual Review of Statistics and Its Application, 7, 11-37.",
    "problem": "Need for systematic approaches to understand and improve scientific practices",
    "assumption_prior_work": "Research quality assessment was primarily subjective and post-hoc",
    "insight": "Meta-research can use quantifiable methodologies to systematically evaluate and improve scientific practices",
    "impact": "Established meta-research as legitimate scientific discipline and influenced research evaluation practices"
  },
  {
    "id": "panofsky-2023-metascience",
    "title": "Metascience as a Scientific Social Movement",
    "authors": "Aaron Panofsky",
    "journal": "Minerva",
    "year": "2023",
    "url": "https://link.springer.com/article/10.1007/s11024-023-09490-3",
    "hypotheses": "Design-stage intervention effectiveness",
    "notes": "Analysis of how metascientists have successfully influenced science policy and practice",
    "strengths": "Sociological analysis, policy impact documentation, institutional change evidence",
    "weaknesses": "Limited to describing rather than predicting future changes",
    "citation": "Panofsky, A. (2023). Metascience as a scientific social movement. Minerva, 61(2), 147-174.",
    "problem": "Understanding why reproducibility crisis gained traction when previous crises did not",
    "assumption_prior_work": "Scientific crises follow similar patterns and have similar outcomes",
    "insight": "Metascience emerged as a scientific social movement combining data science, methodology, and activism",
    "impact": "Explains successful policy changes and provides roadmap for continued reform"
  },
  {
    "id": "valamontes-2025-trim",
    "title": "Meta-Research Meta-Analysis Framework for Evaluating Scientific Fields Across Methods, Reproducibility, and Bias",
    "authors": "Antonios Valamontes, Ioannis Adamopoulos",
    "journal": "Research Preprint",
    "year": "2025",
    "url": "https://www.researchgate.net/publication/391234854",
    "hypotheses": "Universal failure patterns, Cross-field transferability",
    "notes": "TRIM Framework integrating quantitative meta-analysis with reproducibility auditing across standardized indicators",
    "strengths": "Mathematical weighting system, comprehensive framework, reproducibility focus",
    "weaknesses": "Limited empirical validation, complexity of implementation",
    "citation": "Valamontes, A., & Adamopoulos, I. (2025). Meta-Research Meta-Analysis Framework for Evaluating Scientific Fields. ResearchGate preprint.",
    "problem": "Need for systematic evaluation of scientific literature reliability and transparency",
    "assumption_prior_work": "Traditional meta-analysis sufficient without explicit reproducibility assessment",
    "insight": "Reproducibility-centered framework with mathematical weighting can systematically evaluate scientific fields",
    "impact": "Provides practical framework for implementing literature-level quality assessment"
  },
  {
    "id": "nersessian-2022-interdisciplinary",
    "title": "Interdisciplinarity in the Making: Models and Methods in Frontier Science",
    "authors": "Nancy J. Nersessian",
    "journal": "MIT Press",
    "year": "2022",
    "url": "https://mitpress.mit.edu/9780262544665/",
    "hypotheses": "Cross-field transferability, Universal failure patterns",
    "notes": "Comprehensive analysis of how interdisciplinary research works in practice and identifies universal principles",
    "strengths": "Empirical case studies, theoretical framework, practical insights",
    "weaknesses": "Limited to specific case studies, may not generalize broadly",
    "citation": "Nersessian, N. J. (2022). Interdisciplinarity in the Making: Models and Methods in Frontier Science. MIT Press.",
    "problem": "Understanding how successful interdisciplinary research integrates different methodological approaches",
    "assumption_prior_work": "Interdisciplinary research requires completely new methodological approaches",
    "insight": "Successful interdisciplinary work relies on universal principles rather than field-specific techniques",
    "impact": "Provides evidence base for cross-disciplinary methodological transfer"
  },
  {
    "id": "vienni-pohl-2024-knowledge",
    "title": "Exploring Interdisciplinarity and Transdisciplinarity as Knowledge Regimes: A Heuristic Tool for Disentangling Understandings in Academia and Policy",
    "authors": "Bianca Vienni-Baptista, Christian Erik Pohl",
    "journal": "Science, Technology, & Human Values",
    "year": "2024",
    "url": "https://journals.sagepub.com/doi/10.1177/01622439231216789",
    "hypotheses": "Cross-field transferability",
    "notes": "Knowledge regime framework with three components: ideologies/myths, shared beliefs/practices, institutional structures",
    "strengths": "Systematic framework, practical tool, interdisciplinary perspective",
    "weaknesses": "Limited empirical validation, complexity of application",
    "citation": "Vienni-Baptista, B., & Pohl, C. E. (2024). Exploring interdisciplinarity and transdisciplinarity as knowledge regimes. Science, Technology, & Human Values, 49(6), 1147-1175.",
    "problem": "Need for framework to understand and facilitate interdisciplinary collaboration",
    "assumption_prior_work": "Interdisciplinary research understanding limited to project-level analysis",
    "insight": "Knowledge regimes framework can systematically analyze interdisciplinary understanding and practice",
    "impact": "Provides systematic approach for understanding cross-disciplinary knowledge transfer"
  },
  {
    "id": "pinheiro-2021-validation",
    "title": "A large-scale validation of the relationship between cross-disciplinary research and scientific impact",
    "authors": "Henrique Pinheiro, Etienne Vignola-Gagné, David Campbell",
    "journal": "Quantitative Science Studies",
    "year": "2021",
    "url": "https://direct.mit.edu/qss/article/2/2/616/101030",
    "hypotheses": "Cross-field transferability",
    "notes": "Large-scale empirical validation of cross-disciplinary research effectiveness across multiple domains",
    "strengths": "Large sample size, robust methodology, cross-disciplinary scope",
    "weaknesses": "Limited to impact measures, correlation not causation",
    "citation": "Pinheiro, H., Vignola-Gagné, E., & Campbell, D. (2021). A large-scale validation of the relationship between cross-disciplinary research and scientific impact. Quantitative Science Studies, 2(2), 616-642.",
    "problem": "Understanding relationship between cross-disciplinary research and scientific impact",
    "assumption_prior_work": "Cross-disciplinary research benefits assumed but not systematically validated",
    "insight": "Large-scale evidence confirms positive relationship between cross-disciplinary work and scientific impact",
    "impact": "Provides empirical support for investing in cross-disciplinary methodological approaches"
  },
  {
    "id": "ding-2024-criteria",
    "title": "Common criteria for evaluating cross-disciplinary research in global health: a scoping review",
    "authors": "Yan Ding, Jessica Hooper, Imelda Bates",
    "journal": "BMC Global Public Health",
    "year": "2024",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11800405/",
    "hypotheses": "Universal failure patterns, Cross-field transferability",
    "notes": "Systematic identification of common criteria for evaluating research across disciplines in global health context",
    "strengths": "Systematic methodology, practical focus, cross-disciplinary scope",
    "weaknesses": "Limited to global health, may not generalize to all fields",
    "citation": "Ding, Y., Hooper, J., & Bates, I. (2024). Common criteria for evaluating cross-disciplinary research in global health: a scoping review. BMC Global Public Health, 2, 82.",
    "problem": "Lack of common criteria for evaluating research quality across different disciplines",
    "assumption_prior_work": "Quality evaluation must be field-specific and cannot be standardized",
    "insight": "Common evaluation criteria can be identified and applied across disciplinary boundaries",
    "impact": "Demonstrates feasibility of universal evaluation standards for research quality"
  },
  {
    "id": "gebrye-2025-quality",
    "title": "Validation of the Quality Assessment Tool for Systematic Reviews and Meta‐Analyses of Real‐World Studies",
    "authors": "Tadesse Gebrye, Chidozie Mbada, Zalmai Hakimi",
    "journal": "Journal of Evidence Based Medicine",
    "year": "2025",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12203881/",
    "hypotheses": "Universal failure patterns, Design-stage intervention effectiveness",
    "notes": "Development and validation of systematic quality assessment tool applicable across research contexts",
    "strengths": "Rigorous validation process, practical applicability, systematic approach",
    "weaknesses": "Limited to systematic reviews, requires training for implementation",
    "citation": "Gebrye, T., Mbada, C., & Hakimi, Z. (2025). Validation of the Quality Assessment Tool for Systematic Reviews and Meta‐Analyses of Real‐World Studies. Journal of Evidence Based Medicine, 18(2), e70052.",
    "problem": "Need for reliable tools to assess quality of systematic reviews and meta-analyses",
    "assumption_prior_work": "Quality assessment tools are context-specific and cannot be standardized",
    "insight": "Universal quality assessment tools can be developed and validated across different research contexts",
    "impact": "Provides validated tool for systematic quality assessment applicable across research domains"
  },
  {
    "id": "prasad-2024-grade",
    "title": "Introduction to the GRADE tool for rating certainty in evidence and recommendations",
    "authors": "Manya Prasad",
    "journal": "Clinical Epidemiology and Global Health",
    "year": "2024",
    "url": "https://www.sciencedirect.com/science/article/pii/S2213398423002713",
    "hypotheses": "Universal failure patterns, Cross-field transferability",
    "notes": "Systematic approach for rating evidence certainty emphasizing transparency and consistency",
    "strengths": "Widely adopted, systematic approach, proven effectiveness",
    "weaknesses": "Primarily medical focus, complexity of implementation",
    "citation": "Prasad, M. (2024). Introduction to the GRADE tool for rating certainty in evidence and recommendations. Clinical Epidemiology and Global Health, 25, 101484.",
    "problem": "Need for systematic approach to assess evidence certainty and recommendation strength",
    "assumption_prior_work": "Evidence assessment can rely on expert judgment without systematic framework",
    "insight": "Systematic framework can enhance transparency, consistency, and rigor in evidence evaluation",
    "impact": "Established standard for evidence evaluation in healthcare and expanding to other fields"
  },
  {
    "id": "steneck-2024-rigid",
    "title": "Research Integrity in Guidelines and evIDence synthesis (RIGID): a framework for assessing research integrity in guideline development and evidence synthesis",
    "authors": "Nicholas H. Steneck et al.",
    "journal": "EClinicalMedicine",
    "year": "2024",
    "url": "https://www.sciencedirect.com/science/article/pii/S2589537024002967",
    "hypotheses": "Universal failure patterns, Design-stage intervention effectiveness",
    "notes": "Six-step framework for incorporating integrity assessments into guidelines and evidence synthesis with international expert endorsement",
    "strengths": "International validation, systematic approach, practical implementation",
    "weaknesses": "Requires institutional support, resource intensive",
    "citation": "Steneck, N. H., et al. (2024). Research Integrity in Guidelines and evIDence synthesis (RIGID): a framework for assessing research integrity. EClinicalMedicine, 74, 102716.",
    "problem": "Need for systematic incorporation of research integrity assessment into evidence synthesis",
    "assumption_prior_work": "Research integrity assessment can be handled through traditional peer review",
    "insight": "Systematic integrity assessment framework can be integrated into existing evidence synthesis processes",
    "impact": "Provides practical framework for implementing integrity assessment in research evaluation"
  },
  {
    "id": "lin-2025-valid",
    "title": "VALID: A Checklist-Based Approach for Improving Validity in Psychological Research",
    "authors": "Zhicheng Lin et al.",
    "journal": "Research Preprint",
    "year": "2025",
    "url": "https://www.researchgate.net/publication/389205760",
    "hypotheses": "Universal failure patterns, Design-stage intervention effectiveness",
    "notes": "91-item comprehensive checklist developed through Delphi study with 30+ experts, 331 unique versions for different contexts",
    "strengths": "Comprehensive coverage, expert validation, adaptable framework",
    "weaknesses": "Primarily psychological focus, complexity may limit adoption",
    "citation": "Lin, Z., et al. (2025). VALID: A Checklist-Based Approach for Improving Validity in Psychological Research. ResearchGate preprint.",
    "problem": "Need for comprehensive tool to enhance and monitor research validity",
    "assumption_prior_work": "Research validity assessment requires field-specific approaches",
    "insight": "Universal validity checklist can be adapted to diverse research contexts while maintaining core principles",
    "impact": "Provides practical tool for researchers to systematically improve research validity"
  },
  {
    "id": "vanpaemel-2018-curate",
    "title": "A Unified Framework to Quantify the Credibility of Scientific Findings",
    "authors": "Wolf Vanpaemel",
    "journal": "Advances in Methods and Practices in Psychological Science",
    "year": "2018",
    "url": "https://journals.sagepub.com/doi/full/10.1177/2515245918787489",
    "hypotheses": "Universal failure patterns, Design-stage intervention effectiveness",
    "notes": "Curation framework for systematically assessing and quantifying credibility of scientific findings",
    "strengths": "Quantitative approach, systematic framework, practical implementation",
    "weaknesses": "Limited adoption, requires technical expertise",
    "citation": "Vanpaemel, W. (2018). A unified framework to quantify the credibility of scientific findings. Advances in Methods and Practices in Psychological Science, 1(3), 389-402.",
    "problem": "Need for systematic approach to quantify credibility of scientific findings",
    "assumption_prior_work": "Scientific credibility assessment relies primarily on subjective expert judgment",
    "insight": "Systematic curation framework can objectively quantify credibility across different research contexts",
    "impact": "Provides foundation for automated assessment of research credibility and quality"
  },
  {
    "id": "camerer-2016-economics",
    "title": "Evaluating replicability of laboratory experiments in economics",
    "authors": "Colin F. Camerer et al.",
    "journal": "Science",
    "year": "2016",
    "url": "https://science.sciencemag.org/content/351/6280/1433",
    "hypotheses": "Universal failure patterns",
    "notes": "Large-scale replication study in economics finding that only 11 of 18 studies replicated successfully",
    "strengths": "Systematic methodology, economic context, transparent process",
    "weaknesses": "Limited to economics, single replication attempts",
    "citation": "Camerer, C. F., et al. (2016). Evaluating replicability of laboratory experiments in economics. Science, 351(6280), 1433-1436.",
    "problem": "Unknown replication rate in experimental economics and factors affecting reproducibility",
    "assumption_prior_work": "Published economic experiments are generally reliable and replicable",
    "insight": "Economic experiments show similar replication challenges to psychology, suggesting universal patterns",
    "impact": "Extended reproducibility crisis evidence beyond psychology to economics"
  },
  {
    "id": "puetz-2024-earth",
    "title": "The replication crisis and its relevance to Earth Science studies: Case studies and recommendations",
    "authors": "Stephen J. Puetz, Kent C. Condie, Kurt Sundell, Nick M.W. Roberts, Christopher J. Spencer, Slah Boulila, Qiuming Cheng",
    "journal": "Geoscience Frontiers",
    "year": "2024",
    "url": "https://nora.nerc.ac.uk/id/eprint/537372/1/1-s2.0-S1674987124000458-main.pdf",
    "hypotheses": "Universal failure patterns",
    "notes": "Analysis of replication problems in Earth Sciences with identification of key variables for successful replication",
    "strengths": "Earth science focus, practical recommendations, systematic analysis",
    "weaknesses": "Limited to specific types of Earth science studies",
    "citation": "Puetz, S. J., et al. (2024). The replication crisis and its relevance to Earth Science studies: Case studies and recommendations. Geoscience Frontiers, 15(2), 101659.",
    "problem": "Understanding extent and nature of replication problems in Earth Sciences",
    "assumption_prior_work": "Earth Sciences immune to replication problems affecting other fields",
    "insight": "Earth Sciences face similar replication challenges as other fields, with identifiable patterns",
    "impact": "Extended reproducibility crisis documentation to physical sciences"
  },
  {
    "id": "chakravorti-2025-professors",
    "title": "Reproducibility and replicability in research: What 452 professors think in Universities across the USA and India",
    "authors": "Tatiana Chakravorti, Sai Koneru",
    "journal": "PLOS ONE",
    "year": "2025",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC11940819/",
    "hypotheses": "Universal failure patterns",
    "notes": "Large-scale survey of professor attitudes toward reproducibility and replicability across USA and India",
    "strengths": "Large sample, international scope, systematic survey",
    "weaknesses": "Attitude measurement rather than behavior, limited geographic scope",
    "citation": "Chakravorti, T., & Koneru, S. (2025). Reproducibility and replicability in research: What 452 professors think. PLOS ONE, 20(3), e0319334.",
    "problem": "Understanding researcher attitudes toward reproducibility and replicability across cultures",
    "assumption_prior_work": "Researcher attitudes toward reproducibility are uniform across cultures and institutions",
    "insight": "Significant variation in attitudes exists but common concerns transcend cultural boundaries",
    "impact": "Provides baseline for understanding global research culture regarding reproducibility"
  },
  {
    "id": "laraway-2019-behavior",
    "title": "An Overview of Scientific Reproducibility: Consideration of Relevant Issues for Behavior Science/Analysis",
    "authors": "Sean Laraway, Susan Snycerski",
    "journal": "Perspectives on Behavior Science",
    "year": "2019",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC6701706/",
    "hypotheses": "Universal failure patterns",
    "notes": "Analysis of reproducibility issues specific to behavior science with broader implications for scientific reproducibility",
    "strengths": "Field-specific analysis, practical recommendations, systematic approach",
    "weaknesses": "Limited to behavior science, narrow scope",
    "citation": "Laraway, S., & Snycerski, S. (2019). An overview of scientific reproducibility: Consideration of relevant issues for behavior science/analysis. Perspectives on Behavior Science, 42(1), 33-57.",
    "problem": "Understanding reproducibility challenges specific to behavior science and analysis",
    "assumption_prior_work": "Reproducibility issues are primarily relevant to psychology and medical research",
    "insight": "Behavior science faces unique reproducibility challenges that require specialized considerations",
    "impact": "Extended reproducibility crisis analysis to specialized field of behavior science"
  },
  {
    "id": "ioannidis-2023-systematic",
    "title": "The problems with systematic reviews: a living systematic review",
    "authors": "John P.A. Ioannidis",
    "journal": "Journal of Clinical Epidemiology",
    "year": "2023",
    "url": "https://www.sciencedirect.com/science/article/pii/S0895435623000112",
    "hypotheses": "Universal failure patterns, Design-stage intervention effectiveness",
    "notes": "Comprehensive analysis of problems with systematic reviews as research synthesis method",
    "strengths": "Comprehensive analysis, living review format, systematic approach",
    "weaknesses": "Primarily negative focus, limited solutions proposed",
    "citation": "Ioannidis, J. P. A. (2023). The problems with systematic reviews: a living systematic review. Journal of Clinical Epidemiology, 156, 162-171.",
    "problem": "Systematic reviews suffer from methodological problems that undermine their reliability",
    "assumption_prior_work": "Systematic reviews represent gold standard for evidence synthesis",
    "insight": "Systematic reviews have systematic problems that require methodological innovation to address",
    "impact": "Challenges assumptions about systematic review quality and calls for methodological improvements"
  },
  {
    "id": "ankel-peters-2025-incentives",
    "title": "Incentives and the replication crisis in social sciences: A critical review of open science practices",
    "authors": "Johanna Ankel-Peters",
    "journal": "Journal of Economic Surveys",
    "year": "2025",
    "url": "https://www.sciencedirect.com/science/article/abs/pii/S2214804324001642",
    "hypotheses": "Design-stage intervention effectiveness",
    "notes": "Critical analysis of how incentive structures contribute to replication crisis and evaluation of open science solutions",
    "strengths": "Systematic review, incentive analysis, practical focus",
    "weaknesses": "Limited to social sciences, theoretical rather than empirical",
    "citation": "Ankel-Peters, J. (2025). Incentives and the replication crisis in social sciences: A critical review of open science practices. Journal of Economic Surveys, 39(1), 123-158.",
    "problem": "Understanding how academic incentives contribute to replication crisis and evaluating potential solutions",
    "assumption_prior_work": "Replication crisis primarily due to methodological rather than incentive problems",
    "insight": "Incentive structures are fundamental cause of replication problems and must be addressed systematically",
    "impact": "Focuses attention on institutional and incentive reform as necessary for solving reproducibility crisis"
  },
  {
    "id": "ciemer-2025-cris",
    "title": "Introducing CRIS—A unified framework for systematic literature searches across disciplines",
    "authors": "Celina Ciemer, Thomas Jürgen Klotzbier, Sabiha Ghellal",
    "journal": "Frontiers in Public Health",
    "year": "2025",
    "url": "https://pmc.ncbi.nlm.nih.gov/articles/PMC12202348/",
    "hypotheses": "Cross-field transferability, Universal failure patterns",
    "notes": "CRIS framework providing unified methodology for literature searches across disciplines with systematic approach",
    "strengths": "Cross-disciplinary approach, systematic methodology, practical tool",
    "weaknesses": "Limited empirical validation, complexity of implementation",
    "citation": "Ciemer, C., Klotzbier, T. J., & Ghellal, S. (2025). Introducing CRIS—A unified framework for systematic literature searches across disciplines. Frontiers in Public Health, 13, 1489161.",
    "problem": "Need for unified approach to systematic literature searches that works across different disciplines",
    "assumption_prior_work": "Literature search methods must be discipline-specific and cannot be standardized",
    "insight": "Unified framework can systematically guide literature searches across disciplinary boundaries",
    "impact": "Provides practical tool for cross-disciplinary research and evidence synthesis"
  },
  {
    "id": "karunananthan-2024-duplication",
    "title": "Can a replication revolution resolve the duplication crisis in systematic reviews?",
    "authors": "Sathya Karunananthan, Jeremy M Grimshaw, Lara Maxwell, et al.",
    "journal": "Evidence-Based Medicine",
    "year": "2024",
    "url": "https://ebm.bmj.com/content/29/5/285",
    "hypotheses": "Design-stage intervention effectiveness, Universal failure patterns",
    "notes": "Analysis of duplication problems in systematic reviews and potential solutions through replication approaches",
    "strengths": "Systematic analysis, practical focus, evidence-based recommendations",
    "weaknesses": "Limited to systematic reviews, conceptual rather than empirical",
    "citation": "Karunananthan, S., et al. (2024). Can a replication revolution resolve the duplication crisis in systematic reviews? Evidence-Based Medicine, 29(5), 285-291.",
    "problem": "Systematic reviews suffer from duplication problems that waste resources and confuse evidence base",
    "assumption_prior_work": "Multiple systematic reviews on same topic improve evidence base through replication",
    "insight": "Systematic approach to replication can resolve duplication problems while maintaining scientific rigor",
    "impact": "Proposes systematic solution to major problem in evidence synthesis methodology"
  },
  {
    "id": "bruggen-2021-management",
    "title": "Reproducibility and replicability crisis: How management compares to psychology and economics",
    "authors": "Alexander Brüggen, Jeroen Feiks, et al.",
    "journal": "European Management Review",
    "year": "2021",
    "url": "https://www.sciencedirect.com/science/article/pii/S0263237321000025",
    "hypotheses": "Universal failure patterns, Cross-field transferability",
    "notes": "Systematic comparison of replication practices and success rates across management, psychology, and economics",
    "strengths": "Cross-disciplinary comparison, systematic methodology, comprehensive analysis",
    "weaknesses": "Limited to three fields, focus on published replications",
    "citation": "Brüggen, A., et al. (2021). Reproducibility and replicability crisis: How management compares to psychology and economics. European Management Review, 18(3), 191-208.",
    "problem": "Understanding comparative state of replication research across business and social science disciplines",
    "assumption_prior_work": "Replication problems are uniform across social science disciplines",
    "insight": "Different disciplines show varying patterns in replication practices despite similar incentive structures",
    "impact": "Provides comparative evidence for discipline-specific vs. universal approaches to replication problems"
  },
  {
    "id": "wu-2023-psychology",
    "title": "A discipline-wide investigation of the replicability of Psychology papers over the past two decades",
    "authors": "Wu Youyou",
    "journal": "Nature Human Behaviour",
    "year": "2023",
    "url": "https://pubmed.ncbi.nlm.nih.gov/36716367/",
    "hypotheses": "Universal failure patterns",
    "notes": "Large-scale longitudinal analysis of replication patterns in psychology over 20-year period",
    "strengths": "Large scale, longitudinal design, systematic methodology",
    "weaknesses": "Limited to psychology, focus on computational rather than direct replication",
    "citation": "Wu, Y. (2023). A discipline-wide investigation of the replicability of Psychology papers over the past two decades. Nature Human Behaviour, 7(7), 1026-1036.",
    "problem": "Understanding trends in replicability across psychology field over extended time period",
    "assumption_prior_work": "Replication rates in psychology are stable over time",
    "insight": "Systematic patterns in replication success can be identified and tracked over time",
    "impact": "Provides longitudinal perspective on reproducibility crisis and potential for systematic tracking"
  }
]